{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c82a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "#Action = 2\n",
    "#Memory\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from tensorflow import keras\n",
    "import sys\n",
    " \n",
    "# Créer l'environnement Freeway de gymnasium Atari\n",
    "env = gym.make('ALE/Freeway-v5', difficulty=0, mode=0)#, render_mode=\"human\" render_mode=\"human\"\n",
    " \n",
    "# Dimensions de l'image après redimensionnement\n",
    "img_height, img_width = 84, 84\n",
    "img_height, img_width = 174, 84\n",
    " \n",
    " \n",
    "def create_q_model(num_actions):\n",
    "    inputs = layers.Input(shape=(174, 84, 4,))\n",
    " \n",
    "    # Convolutions sur les images à l'écran\n",
    "    layer1 = layers.Conv2D(32, 8, strides=4, activation=\"relu\")(inputs)\n",
    "    layer2 = layers.Conv2D(64, 4, strides=2, activation=\"relu\")(layer1)\n",
    "    layer3 = layers.Conv2D(64, 3, strides=1, activation=\"relu\")(layer2)\n",
    " \n",
    "    layer4 = layers.Flatten()(layer3)\n",
    " \n",
    "    layer5 = layers.Dense(512, activation=\"relu\")(layer4)\n",
    "    action = layers.Dense(num_actions, activation=\"linear\")(layer5)\n",
    " \n",
    "    return keras.Model(inputs=inputs, outputs=action)\n",
    " \n",
    "# Fonction pour entraîner le modèle\n",
    "def model_loss(target, network, action):\n",
    "    lossess = tf.square(target - network) # action\n",
    "    return tf.reduce_mean(lossess* action)\n",
    "  \n",
    "def reward_policy(reward, ob, action):\n",
    "    if reward == 1:\n",
    "        reward = 1\n",
    "    elif ob[16] == 1:  # Collision!\n",
    "        reward = -1\n",
    "    elif action != 1:  # Ne pas l'inciter à rester immobile\n",
    "        reward = -0.25\n",
    " \n",
    "    return reward\n",
    "\n",
    "def reduce_state(ob):\n",
    "    # Peut importe où on a été touché\n",
    "    ob[16] = 1 if ob[16] != 255 else 0\n",
    " \n",
    "    # Réduit la position du poulet\n",
    "    ob[14] = ob[14] // 3\n",
    " \n",
    "    for b in range(108, 118):\n",
    "        if ob[b] < 20 or ob[b] > 80:\n",
    "            # Pas besoin de représenter les voitures qui sont loins des poulets\n",
    "            ob[b] = 0\n",
    "        else:\n",
    "            ob[b] = ob[b] // 3\n",
    " \n",
    "    return ob\n",
    " \n",
    "def preprocess_observation(observation):\n",
    "    crop = tf.image.crop_to_bounding_box(observation, 20, 8, 174, 84)\n",
    "    grayscale = tf.image.rgb_to_grayscale(crop)\n",
    "    normalized_image = tf.image.per_image_standardization(grayscale)\n",
    "    return normalized_image   \n",
    " \n",
    "    \n",
    " \n",
    "# Boucle principale d'entraînement\n",
    "def train_freeway():\n",
    "    RAM_mask = [\n",
    "      14  # Chicken Y\n",
    "    , 16  # Chicken Lane Collided\n",
    "    , 108, 109, 110, 111, 112, 113, 114, 115, 116, 117  # Car X Coords\n",
    "    ]\n",
    "    model = create_q_model(2)\n",
    "    model_target = create_q_model(2)\n",
    "    optimizer = keras.optimizers.RMSprop(learning_rate=0.0001, rho=0.99)\n",
    "\n",
    "    eps = 1.0  # Epsilon greedy parameter\n",
    "    epsilon_min = 0.1  # Minimum epsilon greedy parameter\n",
    "    epsilon_max = 1.0  # Maximum epsilon greedy parameter\n",
    "    epsilon_interval = (epsilon_max - epsilon_min)  # Rate at which to reduce chance of random action being taken\n",
    "    epsilon_greedy_frames = 50\n",
    "\n",
    "    batch_size = 32\n",
    "    rewards_per_episode = []\n",
    "    loss_per_episode = []\n",
    "    states, rewards, next_states, actions = [], [], [], []\n",
    "    acte = 0\n",
    "    score_plus_epi = []\n",
    "    score_plus_epi_tot = []\n",
    "    count = 0\n",
    "    #Nombre d'épisodes (époques)\n",
    "    for epi in range(100):\n",
    "        print(\"----------------------------new episode----------------------------\")\n",
    "        # Rediriger la sortie standard vers os.devnull\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        score_plus = []\n",
    "        obs, info = env.reset()\n",
    "        st = preprocess_observation(obs)\n",
    "        memory = [st,st,st,st]\n",
    "        count = 0\n",
    "        while True:\n",
    "            print(count)\n",
    "            memory_array = np.array(memory)\n",
    "            memory_array_rgb = np.concatenate(memory_array, axis=-1)\n",
    "            if count % 4 == 0:\n",
    "                if np.random.rand() < eps:\n",
    "                    acte =  np.random.randint(2)\n",
    "                else:\n",
    "                    q_values = model.predict(np.expand_dims(memory_array_rgb, axis=0)) \n",
    "                    acte = np.argmax(q_values)\n",
    "                if acte == 0:\n",
    "                    act = 2\n",
    "                else :\n",
    "                    act = 1\n",
    "            observation, reward, terminated, truncated, info = env.step(act)\n",
    "            st2 = preprocess_observation(observation)\n",
    "            ram_state = env.unwrapped.ale.getRAM()\n",
    "            ram_state = reduce_state(ram_state)\n",
    "            reward = reward_policy(reward, ram_state, acte)\n",
    "            score_plus.append(reward)\n",
    "\n",
    "            mask = np.zeros(2)\n",
    "            mask[acte] = 1\n",
    " \n",
    "            index = np.random.randint(len(states) + 1)\n",
    "            memory_array = np.array(memory)\n",
    "            memory_array_rgb = np.concatenate(memory_array, axis=-1)\n",
    "            states.insert(index, memory_array_rgb)\n",
    "            rewards.insert(index, reward)\n",
    "            memory.pop(0)\n",
    "            memory.append(st2)\n",
    "            memory_array = np.array(memory)\n",
    "            memory_array_rgb = np.concatenate(memory_array, axis=-1)\n",
    "            next_states.insert(index, memory_array_rgb)\n",
    "            actions.insert(index, mask)\n",
    " \n",
    "            if len(states) > 10000:\n",
    "                states.pop(0)\n",
    "                rewards.pop(0)\n",
    "                next_states.pop(0)\n",
    "                actions.pop(0)\n",
    " \n",
    "            #st = st2\n",
    "            count +=1\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "        print(\"---------------\")\n",
    "        print(len(states))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        size = len(next_states)\n",
    "\n",
    "        # Transform each array into a tensor\n",
    "        tf_states = tf.convert_to_tensor(np.array(states), dtype=tf.float32)\n",
    "        tf_rewards = tf.convert_to_tensor(np.array(rewards), dtype=tf.float32)\n",
    "        tf_next_states = tf.convert_to_tensor(np.array(next_states), dtype=tf.float32)\n",
    "        tf_actions = tf.convert_to_tensor(np.array(actions), dtype=tf.float32)\n",
    " \n",
    "        # Get the QTargets\n",
    "        Q_stp1 = model_target.predict(tf_next_states)\n",
    "        Qtargets = tf.convert_to_tensor(tf_rewards.numpy().reshape(-1, 1) + 0.99 * np.max(Q_stp1, axis=1).reshape(size, 1))\n",
    " \n",
    "        # Generate batch of training and train the model\n",
    "        losses = []\n",
    " \n",
    "        for b in range(0, size, batch_size):\n",
    "            with tf.GradientTape() as tape:\n",
    "                to = min(b + batch_size, size)\n",
    "                tf_states_b = tf_states[b:to]\n",
    "                tf_actions_b = tf_actions[b:to]\n",
    "                Qtargets_b = Qtargets[b:to]\n",
    " \n",
    "                predictions_network = model(tf_states_b)\n",
    "                loss = model_loss(Qtargets_b, predictions_network, tf_actions_b)\n",
    "                losses.append(loss.numpy())\n",
    " \n",
    "            # Calculate gradients and update the model\n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            #model.\n",
    "        if epi % 3 == 0:\n",
    "            # update the the target network with new weights\n",
    "            model_target.set_weights(model.get_weights())\n",
    "            # Log details\n",
    "            template = \"running reward: {:.2f} at episode {}\"\n",
    "            print(template.format(np.mean(rewards), epi))     \n",
    "\n",
    "        sys.stdout = original_stdout\n",
    "        print(\"Mean loss\", np.mean(losses))\n",
    "        loss =  np.mean(losses)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        rewards_per_episode.append(sum(rewards))\n",
    "        loss_per_episode.append(loss)\n",
    "        print(\"Episode {}: Total Reward: {}, Epsilon: {:.4f}\".format(epi + 1, sum(rewards), eps))\n",
    "        print(\"Rewards mean:\", np.mean(rewards))\n",
    "        somme_positifs_alternative = sum(nombre for nombre in score_plus if nombre > 0)\n",
    "        score_plus_epi.append(somme_positifs_alternative)\n",
    "        score_plus_epi_tot.append(sum(score_plus))\n",
    "        print(\"Score du jeux:\", somme_positifs_alternative)\n",
    "        print(\"Score des récompenses:\", sum(score_plus))\n",
    "        if sum(score_plus)>25:\n",
    "            print(\"Solved at episode {}!\".format(epi))\n",
    "            #break\n",
    "        eps -= epsilon_interval / epsilon_greedy_frames\n",
    "        eps = max(eps, epsilon_min)\n",
    "\n",
    "    plt.figure()\n",
    "    # Graphique des récompenses par épisode\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(rewards_per_episode)\n",
    "    plt.xlabel('Épisode')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.title('Récompenses de la mémoire par épisode')\n",
    "    # Graphique de la perte moyenne par épisode\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(loss_per_episode)\n",
    "    plt.xlabel('Épisode')\n",
    "    plt.ylabel('Mean Loss')\n",
    "    plt.title('Loss moyenne par épisode')\n",
    "    # Ajuster l'espace entre les sous-graphiques\n",
    "    plt.tight_layout()\n",
    "    # Afficher la figure\n",
    "    plt.savefig('Figure_1.png')  \n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(score_plus_epi)\n",
    "    plt.xlabel('Épisode')\n",
    "    plt.ylabel('Total score')\n",
    "    plt.title('Score du jeux')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(score_plus_epi_tot)\n",
    "    plt.xlabel('Épisode')\n",
    "    plt.ylabel('Total score')\n",
    "    plt.title('Score des récompenses')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('Figure_2.png')\n",
    "    plt.show()\n",
    "\n",
    "    output_dir = 'model_output/'\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    model.save(output_dir + \"Model.h5\")\n",
    " \n",
    "# Appeler la fonction principale d'entraînement\n",
    "train_freeway()\n",
    "# Fermer l'environnement\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential,load_model\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    " \n",
    " \n",
    "img_height, img_width = 174, 84 #img_height, img_width = 174, 84\n",
    "def preprocess_observation(observation):\n",
    "    crop = tf.image.crop_to_bounding_box(observation, 20, 8, 174, 84)\n",
    "    grayscale = tf.image.rgb_to_grayscale(crop)\n",
    "    normalized_image = tf.image.per_image_standardization(grayscale)\n",
    "    return normalized_image\n",
    " \n",
    " \n",
    "env = gym.make('ALE/Freeway-v5', render_mode=\"human\", difficulty=0, mode=0)\n",
    " \n",
    " \n",
    " \n",
    " \n",
    "state, info = env.reset()\n",
    " \n",
    " \n",
    "output_dir = 'model_output/'\n",
    " \n",
    "model = load_model(output_dir + \"Model.h5\")\n",
    " \n",
    "# Paramètres pour la simulation de test\n",
    "test_episodes = 5\n",
    " \n",
    "# Boucle pour les épisodes de test\n",
    "for episode in range(test_episodes):\n",
    "    state, info = env.reset()\n",
    "    state = preprocess_observation(state)\n",
    "    memory = [state,state,state,state]\n",
    "    total_reward = 0\n",
    "    count =0\n",
    "    while True:\n",
    "        original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        # Utiliser le modèle pour prendre une action\n",
    "        memory_array = np.array(memory)\n",
    "        memory_array_rgb = np.concatenate(memory_array, axis=-1)\n",
    "\n",
    "        action = model.predict(np.expand_dims(memory_array_rgb, axis=0))\n",
    "        action = np.argmax(action[0])\n",
    "        sys.stdout = original_stdout\n",
    "\n",
    "        if action == 0:\n",
    "           action = 2\n",
    "\n",
    "        # Appliquer l'action à l'environnement\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        next_state = preprocess_observation(next_state)\n",
    "        memory.pop(0)\n",
    "        memory.append(next_state)\n",
    "        total_reward += reward\n",
    "        count +=1\n",
    "        if done:\n",
    "            print(\"Episode {}: Total Reward: {}\".format(episode + 1, total_reward))\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c54a86b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
